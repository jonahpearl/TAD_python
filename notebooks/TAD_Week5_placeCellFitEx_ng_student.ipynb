{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iQWX4xSYZEq"
   },
   "source": [
    "# TAD Week 5: Place Cells"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "metadata": {},
   "source": [
    "### Test header that jonah inserted again"
   ]
  },
  {
   "cell_type": "markdown",
>>>>>>> 11acbb9 (small test change to week 5)
   "metadata": {
    "id": "nnQOCVwxYd-F"
   },
   "source": [
    "This exercise has been adapted from:\n",
    "Chapter 9, of \"Case studies in neural data analysis\" by Kramer & Eden 2016\n",
    "\n",
    "What to do: Login to Learning Catalytics (LC) and join the session for\n",
    "the module entitled \"placeCellFit\". You will answer a series of questions\n",
    "based on the guided programming below. \n",
    "Read through the comments and follow the instructions provided. In some\n",
    "cases you will be asked to answer a question, clearly indicated by\n",
    "'QUESTION' and a corresponding 'Q#' that directs you to answer the\n",
    "relevant question in LC. In other cases, you be asked to supply missing\n",
    "code, indicated by 'TODO'. \n",
    "\n",
    "The 1st two sections of code don't require you to do anything, but you\n",
    "still need to execute them in order to load the data and display an\n",
    "initial plot.\n",
    "\n",
    "Original source of exercise:\n",
    "Mark A. Kramer and Uri T. Eden, \"Case Studies in Neural Data Analysis\",\n",
    "MIT Press, 2016, Chapter 9.\n",
    "https://mitpress.mit.edu/books/case-studies-neural-data-analysis\n",
    "\n",
    "Adapted by RTB, 9 Dec. 2016\n",
    "Developed for homework by LD and RTB, March-April 2017\n",
    "Refined by RTB, September 2017. Adapted by EB to Python September 2021. \n",
    "\n",
    "**Concepts covered:**\n",
    "1. Working with spike data: spike times to sparse matrix indices\n",
    "2. Occupancy normalized histogram for place fields\n",
    "3. Using glmfit to form a Poisson point-process model\n",
    "4. Model selection through analysis of residuals\n",
    "5. Model comparison through measures of goodness of fit: AIC,\n",
    "      Chi-square, parameter CIs, Kolmogorov-Smirnov plot\n",
    "\n",
    "Scenario: A rat runs back and forth in a linear maze, incentivized by a\n",
    "morsel of chocolate each time she reaches the end of the maze. While she\n",
    "is running we are recording the action potentials from a single neuron in\n",
    "her hippocampus.\n",
    "\n",
    "\n",
    "Scientific question: What aspects of the rat's behavior influence the\n",
    "neuron's tendency to fire an action potential?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrdbIXarYvOE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0R86R42fwzi"
   },
   "source": [
    "# Loading & plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBLFrc_NYqYh"
   },
   "source": [
    "## Load data\n",
    "\n",
    "We will have a pandas dataframe with time bins as rows. The column `Time [s]` is  time axis for entire experiment (in seconds at 1 ms resolution). The column `Position [cm]` is the rat's position (in cm) at each time point.\n",
    "\n",
    "`spike_times` is a numpy array consisting of the times at which each recorded action potential occurred (in seconds at 1 ms resolution). It's not yet in the pandas dataframe because it is not in a time-bin based format (spoilers: we'll put it in this format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5VsyKVIYUeG"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1a0aeagYMFS-ZcQyx8hJgnMwMd9HlnK13\n",
    "\n",
    "# These are just for figure layout:\n",
    "nr = 4\n",
    "nc = 3\n",
    "\n",
    "# Load in matlab file\n",
    "matlab_data = sio.loadmat('/content/placeCellData.mat')\n",
    "\n",
    "spike_times = matlab_data['spikeTimes'][:, 0]\n",
    "\n",
    "# Create pandas dataframe\n",
    "data = pd.DataFrame({'Time [s]': matlab_data['expTime'][:, 0], 'Position [cm]': matlab_data['ratPosition'][:, 0]})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUwX2iJYaLqg"
   },
   "source": [
    "## Plot rat's position over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1bFfcOXZyTr"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]')\n",
    "\n",
    "ax.set(title = 'Fig. 1: Rat position vs. time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEEtH8cYaJGg"
   },
   "source": [
    "Look at the 3 experimental variables and make sure you understand what\n",
    "each one means and how the data are formatted.\n",
    "\n",
    "**QUESTION (Q1)**: \n",
    "What was the duration of the entire experiment, in seconds (rounded to\n",
    "nearest second)?\n",
    "\n",
    "**QUESTION (Q2)**:\n",
    "How many action potentials did the hippocampal neuron fire during the\n",
    "entire experiment?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GF2XpW2azIy"
   },
   "source": [
    "## Plot spikes on top of position trace\n",
    "\n",
    "We want to know where in the maze the rat was each time a spike was fired\n",
    "by our neuron. Currently, all we know is the *time* that each spike was\n",
    "fired. But we also know that rat's position at every moment in time, so\n",
    "we can use these two pieces of information to derive what we want to\n",
    "know. Because we are using these variables at subsequent stages of the\n",
    "exercise, we will generate them sequentially.\n",
    "\n",
    "TODO: Make a new binary column in the pandas dataframe which has a 1 at each\n",
    "time point where the neuron fired a spike and a 0 elsewhere. Name that\n",
    "column `Spikes`. Use the space provided below. \n",
    "\n",
    "Hint: use the columns `Time [s]` and the array `spike_times`.\n",
    "\n",
    "Hint #2: `data['Spikes'].sum()` should be\n",
    "the total number of spikes. This will be the critical information for our subsequent regression.\n",
    "\n",
    "Hint #3: you may want to use `np.histogram`. This takes in the data you want to bin and the edges of bins. You may need to go from an array that gives you the center values of the bins to one that gives you the edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVZLM17RajvM"
   },
   "outputs": [],
   "source": [
    "... # your code here\n",
    "data['Spikes'] = ...\n",
    "\n",
    "data['Spikes'] = data['Spikes'].astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUkXf-4kcow3"
   },
   "source": [
    "We can recover the spike times by indexing in with a boolean `Spikes` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7NJyslWjpHi"
   },
   "outputs": [],
   "source": [
    "data['Time [s]'].loc[data['Spikes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSCfqqR4c4tP"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]', ax = ax)\n",
    "sns.scatterplot(data = data.loc[data['Spikes']], x = 'Time [s]', y = 'Position [cm]',\n",
    "                color = 'r')\n",
    "\n",
    "ax.set(title = 'Fig. 1: Rat position vs. time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmgDKGpDdmbV"
   },
   "source": [
    "**QUESTION (Q3)**: When does the cell fire? Is it just a place cell?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmK3WmNGdsGw"
   },
   "source": [
    "## Occupancy normalized histogram\n",
    "\n",
    "We want to visualize the probability of the cell firing as a function of\n",
    "position along the maze (ignoring for now the directionality issue).\n",
    "Because the rat may not be moving at a perfectly constant speed, it\n",
    "potentially spends more or less time in each spatial bin, so we need to\n",
    "normalize by the amount of time she spends in each bin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fzDQP4Gc5UY"
   },
   "outputs": [],
   "source": [
    "position_bin_edges = np.arange(np.round(data['Position [cm]'].min()) - 5, np.round(data['Position [cm]'].max()) + 6, 10)\n",
    "\n",
    "position_binned_data = pd.DataFrame({\n",
    "    'Position bins [cm]': (position_bin_edges[:-1] + position_bin_edges[1:])/2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK1XM4bBeAvZ"
   },
   "source": [
    "TODO: Using the `position_bin_edges` indicated above, make a histogram of positions where we got spikes. \n",
    "\n",
    "NOTE: You need to both plot a histogram and create a variable containing the spike-counts per bin, which we will use later. Make this a column in the pandas data frame `position_binned_data`. We will first get the bin counts using `np.histogram` and then will use `sns.bar` to plot a histogram from the bin counts. We could use `sns.hist` but we want the bin counts too. \n",
    "\n",
    "`plt.hist` both plots the histogram and returns the spike-counts per bin but we will just use `np.histogram` for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwlBa6fk2mA6"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Spike count / bin'] = ...\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Spike count / bin')\n",
    "ax.set(title = 'Spike histogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGHrK2IqiDra"
   },
   "source": [
    "TODO: Using the `position_bin_edges` indicated above, make a histogram of the\n",
    "occupancy times in seconds. Think carefully about what you are binning\n",
    "here. If, for example, a given position bin contains 100 counts (from the\n",
    "column `Position [cm]`), how many seconds did the rat spend in that position\n",
    "bin? Since we need a step to convert from counts to seconds, we will get the counts using `np.histogram` and then plot using `plt.bar`. As a reality check, you can see from fig. 1 that the entire\n",
    "experiment lasted just shy of 180 seconds. If you have calculated the\n",
    "occupancy times in seconds, then the sum of all occupancy bins should add\n",
    "up to the total length of the experiment. Name this variable\n",
    "`occupancy_hist`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEgGVBYN3kFG"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Time / bin [s]'] = ...\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Time / bin [s]')\n",
    "ax.set(title = 'Position histogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztGiTa1SmR3g"
   },
   "source": [
    "Now make a histogram of the positions where spikes occurred that is \n",
    "adjusted for the rat's occupancy time in each position bin. To do this,\n",
    "we simply divide our spike counts, `Spike counts` by the time per bin, `Time / bin [s]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUi1EMS3l1qR"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Occupancy rate [sp/s]'] = position_binned_data['Spike count / bin'] / position_binned_data['Time / bin [s]']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]')\n",
    "ax.set(title = 'Occupancy normalized histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLl0idXqmu4h"
   },
   "source": [
    "Let's plot these prior three histograms together to visualize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftDtgZZFl2qZ"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize = (12, 4))\n",
    "\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Spike count / bin', ax = axes[0])\n",
    "axes[0].set(title = 'Spike histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Time / bin [s]', ax = axes[1])\n",
    "axes[1].set(title = 'Position histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]', ax = axes[2])\n",
    "axes[2].set(title = 'Occupancy normalized histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVEmYAS-ms4I"
   },
   "source": [
    "**QUESTION (Q4)**: Compare the histogram on the left ('Spike histogram') with the one on the right ('Occupancy normalized  histogram'). Are there any differences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHYneqf9f2uI"
   },
   "source": [
    "# Fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5yZ2hG7nYQL"
   },
   "source": [
    "## Model #1: Position as a covariate\n",
    "\n",
    "We want to fit a model that will predict the cell's spike counts in each\n",
    "bin as a function of its position along the track. The natural model \n",
    "is the Poisson, where we express the mean rate as a function of time in\n",
    "terms of the covariates: lambda_t = beta0 + beta_1(position_t)\n",
    "\n",
    "However, as we discussed in lecture, the right side of our equation is\n",
    "not bounded and can assume negative values, whereas spike rates cannot go\n",
    "below 0. So the trick is to use a so-called 'link function' to transform\n",
    "our dependent variable--in this case we use the natural logarithm\n",
    "('log'), so that we are actually fitting log(lambda_t). The generalized\n",
    "linear model makes this very easy. We don't even have to overtly take\n",
    "the log of our dependent variable; we just treat it like ordinary linear\n",
    "regression, but, in addition, specifiy the appropriate probability\n",
    "distribution (in our case, 'poisson') and the appropriate link function\n",
    "('log'). But we must remember that we are still really fitting\n",
    "log(lambda_t) if we are to interpret our beta coefficients properly.\n",
    "\n",
    "TODO: Fit a Poisson Model to the spike train data using the rat's\n",
    "position as a predictor. Fill in the inputs below. See help on the `GLM` function in statsmodel: https://www.statsmodels.org/stable/glm.html#module-reference. \n",
    "\n",
    "Hint: We always want to fit the 'rawest' form of the data,\n",
    "which, in this case is the rat's position at every ms (independent\n",
    "variable) and whether or not the neuron fired a spike at every ms\n",
    "(dependent variable).\n",
    "\n",
    "Hint: don't forget that we want a constant term as well, remember to use `sm.add_constant`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bI8kqkJp34Su"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.GLM(...)\n",
    "results1 = model.fit()\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THXjjpoGoBLT"
   },
   "source": [
    "THOUGHT QUESTION: What is contained in the `results1` oabject? What is in `results1.params`\n",
    "\n",
    "Interpreting our beta coefficients is a bit trickier now, because they\n",
    "are predicting log(lambda_t), not lambda_t directly, and it is the latter\n",
    "in which we are interested. So in order to interpret the coefficients in\n",
    "a straightforward way, we need to 'undo' the natural logarithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIsyEb8p5Pdl"
   },
   "source": [
    "**QUESTION (Q5)**: What is our predicted firing rate when the rat is at position 0? Give your answer in spikes/sec\n",
    "to 1 decimal place.\n",
    "\n",
    "HINT: Write down the model!!\n",
    "\n",
    "HINT #2: Think about the time-scale of the variables\n",
    "that went into the model (i.e. position or spiking at each millisecond) and the units you are being asked to provide your answer in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kynhRSe15px_"
   },
   "outputs": [],
   "source": [
    "rate0 = ...\n",
    "\n",
    "print(rate0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8oXkn607j4Q"
   },
   "source": [
    "Let's re-plot the occupancy norm. hist. with our model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKVTvndH89Du"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Model 1 predictions'] = results1.predict(sm.add_constant(position_binned_data['Position bins [cm]'])) * 1000\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]')\n",
    "ax.plot(position_binned_data['Position bins [cm]']/10, position_binned_data['Model 1 predictions'], 'r')\n",
    "ax.set(title = 'Model 1: Position only covariate', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMQ8fTzv7BnT"
   },
   "source": [
    "Note we could have also used sklean, as below, which is an even more popular Python library. We use statsmodel because it has a stronger emphasis on statistical testing (provides p values, confidence intervals etc of parameter estimation). The `PoissonRegressor` class is in a very new version of sklean, so you'd need to uncomment the cell below, run it to upgrade sklearn, and then restart the runtime of the notebook if you want to play around with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QjIPjck8PFm"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KP9apQQl73_n"
   },
   "outputs": [],
   "source": [
    "# import sklearn.linear_model \n",
    "\n",
    "# sklearn_model = sklearn.linear_model.PoissonRegressor(alpha = 0, fit_intercept = True)\n",
    "\n",
    "# sklearn_model.fit(data['Position [cm]'][:, None], data['Spikes'])\n",
    "\n",
    "# print(sklearn_model.coef_)\n",
    "# print(sklearn_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X6LHkcG8FB4"
   },
   "source": [
    "## Model #2: Adding position-squared as a covariate\n",
    "\n",
    "We see that our first model does a very poor job of describing the data.\n",
    "This would have been obvious with a little thought once we wrote down our\n",
    "model: the first model requires the log of the spike rate to be a linear\n",
    "function of the rat's position, so it can only get larger as the position\n",
    "increases. But we see that the spike rate goes up until the rat is\n",
    "centered at about the 60 or 70 cm bin, and then goes down. When one sees\n",
    "this kind of behavior (i.e. non-monotonic in 'x'), a standard trick is to\n",
    "include an additional co-variate that is related to x-squared. Why we use\n",
    "this trick should become more evident below. But for now, let's just do\n",
    "it and see what happens.\n",
    "\n",
    "TODO: Improve model fit by ADDING a squared term for position.\n",
    "Hint: `results2.params` should contain 3 elements: an intercept, a coefficient for\n",
    "position and a coefficient for position-squared.\n",
    "\n",
    "Hint: adding each new input term as a column in `data` can be useful for specifying inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prtT_-eHZ3Pi"
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "model = sm.GLM(...)\n",
    "\n",
    "results2 = model.fit()\n",
    "print(results2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SKUVwls8e4N"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Model 2 predictions'] = results2.predict(sm.add_constant(pd.concat((position_binned_data['Position bins [cm]'], \n",
    "                                                                                          position_binned_data['Position bins [cm]']**2), axis = 1))) * 1000\n",
    "                                                                                          \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]')\n",
    "ax.plot(position_binned_data['Position bins [cm]']/10, position_binned_data['Model 2 predictions'], 'r')\n",
    "ax.set(title = 'Model 2: Add Position^2 as covariate', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVihomHyadRC"
   },
   "source": [
    "## Re-cast the model for easier interpretation of beta coefficients\n",
    "\n",
    "Notice that model #2 looks sort of like a Gaussian. And if we compare\n",
    "the model to the formula for a Gaussian, we notice that we can transform\n",
    "one into the other:\n",
    "\n",
    "Gaussian: lambda_t = alpha * exp((ratPosition - mu).^2 / (2*sigma.^2))\n",
    "\n",
    "Model 2: lambda_t = exp(beta0 + beta1 * ratPosition + beta2 * ratPosition^2);\n",
    "\n",
    "We've essentially replaced our 3 beta terms with 3 new terms (alpha, mu\n",
    "and sigma) that correspond to more intuitive concepts. With a little\n",
    "algebra, we can express the 3 Gaussian parameters in terms of our beta\n",
    "parameters.\n",
    "\n",
    "(As a cool parenthetical, it turns out from statistical theory that the\n",
    "maximum likelihood estimate (MLE) of any function of the fit parameters\n",
    "is the just the same function applied to the MLE parameters.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNc1-mRmaSFH"
   },
   "outputs": [],
   "source": [
    "# Compute maximum likelihood estimates of:\n",
    "\n",
    "# Place field center\n",
    "mu = -results2.params[1] / 2 / results2.params[2]\n",
    "\n",
    "# Place field size\n",
    "sigma = np.sqrt(-1 / ( 2 * results2.params[2]))   \n",
    "\n",
    "# Max firing rate\n",
    "alpha = np.exp(results2.params[0] - results2.params[1]**2 / 4 / results2.params[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-eoNKC3bT24"
   },
   "source": [
    "## Bonus: Gaussian fit directly with unconstrained nonlinear optimization\n",
    "\n",
    "NOTE: You don't have to do anything in this cell. Just read the comments\n",
    "and then execute the code. \n",
    "\n",
    "At this point, you might be asking yourself, \"Why go through all of that\n",
    "algebraic gymnastics? Why not just fit a Gaussian model directly?\" Well,\n",
    "we can do this using `scipy.optimize.minimize`. What we need to do is\n",
    "write an objective function, call it 'fit_fun_gauss2', that will take in 3\n",
    "parameters and return some measure of error in the fit to the actual\n",
    "data. The 3 input parameters should be:\n",
    "\n",
    "`q` : a vector of our 3 parameters (q[0]=alpha, q[1]=mu, q[2]=sigma)\n",
    "\n",
    "`data['Position [cm]']` : our independent variable (x)\n",
    "\n",
    "`data['Spikes']` : our dependent variable (y)\n",
    "\n",
    "This function should first use the parameters to calculate values of\n",
    "lambda from position as predicted by the current model (i.e. the\n",
    "parameters in 'q'). Then it should compute the likelihood of the actual\n",
    "values ('spikes') given the model's current estimates of the lambdas,\n",
    "and finally return the fit error as -log of the sum of these likelihoods:\n",
    "\n",
    "  -sum(log(scipy.stats.poisson.pmf(y,lambda)))\n",
    "\n",
    "We then use 'scipy.optimize.minimize' to find the parameters, q, that minimize the\n",
    "value returned by 'fit_fun_Gauss2'.\n",
    "\n",
    "THOUGHT QUESTION: Why do we have our function return *minus* the sum of\n",
    "the log likelihood? HINT: Think about what 'scipy.optimize.minimize' is doing and the\n",
    "overall goal, which is to maximize the likelihood of our data given the\n",
    "model.\n",
    "\n",
    "ANSWER: We are calculating a maximum likelihood estimate for our\n",
    "paramters by minimizing -log(likelihood) of the model given the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuVotAl9tU6W"
   },
   "outputs": [],
   "source": [
    "def fit_fun_Gauss2(q, x, y):\n",
    "  \"\"\"Compute error given Gaussian parameters and data\n",
    "\n",
    "  Args:\n",
    "    q: numpy array of size (3,) with Gaussian parameters, q[0] = alpha, q[1] = mu, q[2] = sigma\n",
    "    x: pandas series of independent variable\n",
    "    y: pandas series of dependent variable\n",
    "  \"\"\"\n",
    "\n",
    "  lambda_preds = q[0] * np.exp(-(((x - q[1]))**2) / (2 * q[2] ** 2))\n",
    "\n",
    "  # Negative log likelihood\n",
    "  err = -np.sum(np.log(scipy.stats.poisson.pmf(y, lambda_preds)))\n",
    "\n",
    "  return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgkEerqMuycb"
   },
   "outputs": [],
   "source": [
    "# Look at our occupancy normalized histogram to generate guesses:\n",
    "\n",
    "# Reasonable guesses for alpha, mu, sigma\n",
    "q0 = np.array([20/1000, 30, 30])  \n",
    "\n",
    "qit = scipy.optimize.minimize(fit_fun_Gauss2, q0, args = (data['Position [cm]'].values, data['Spikes'].values), method = 'Nelder-Mead')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxpOJX-SwrUE"
   },
   "outputs": [],
   "source": [
    "# Compare fits with alpha, mu and sigma calculated above\n",
    "print([alpha, mu, sigma])\n",
    "\n",
    "# Silly print option stuff so we don't see the scientific notation version\n",
    "np.set_printoptions(suppress=True)\n",
    "print(qit.x)\n",
    "np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x10hqKqytrv5"
   },
   "source": [
    "But to see the down side of this approach, try making initial guesses\n",
    "that are less well guided by the histogram (e.g. q0 = [0,0,0]). \n",
    "Do we always converge to the correct answer? Ans. No\n",
    "\n",
    "THOUGHT QUESTION: Can you think of other benefits of using the GLM\n",
    "approach?\n",
    "\n",
    "ANSWER:  statsmodel is faster, we are guaranteed to get the right answer, and\n",
    "we automatically get all sorts of useful information, such as standard\n",
    "errors and confidence intervals. But note that we could get standard\n",
    "errors and CIs by bootstrapping if we chose to use the direct fit of a\n",
    "Gaussian model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W2b7PJJbhEB"
   },
   "source": [
    "## Analysis of residuals\n",
    "\n",
    "Residuals tell us, on a point-by-point basis, the difference between the\n",
    "data and the predictions of our model. `sm.GLM` gives us these values for\n",
    "free, and they are a valuable resource for evaluating deficiencies in our\n",
    "model. One type of residual that is particularly useful for spike data is\n",
    "the cumulative raw residual, which is just the sum of the residuals up to\n",
    "each point in time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8GTtYAFbMb2"
   },
   "outputs": [],
   "source": [
    "data['Model 2 Cum. Residuals'] = np.cumsum(results2.resid_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFUDiJP-Atus"
   },
   "outputs": [],
   "source": [
    "# Superimpose cumulative residuals on rat position over time\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 5))\n",
    "\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]', color = '#D95319', lw = .5, ax = ax)\n",
    "ax.yaxis.label.set_color('#D95319')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Model 2 Cum. Residuals', color = '#0072BD', lw = .5, ax = ax2)\n",
    "ax2.yaxis.label.set_color('#0072BD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfiyzlIqc4Ti"
   },
   "source": [
    "**QUESTION (Q6)**: Describe the relationship between the residuals of our\n",
    "model and the position of the rat over time.\n",
    "\n",
    "**QUESTION (Q7)**: Describe the source of this relationship. Think about what\n",
    "the current model \"knows\" (i.e. what predictor variables it contains) and\n",
    "what the cell is actually doing.\n",
    "\n",
    "**QUESTION (Q8)**: If we had the \"correct\" model, what should the cumulative\n",
    "residuals look like in a similar plot?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pkCqoYAdBi-"
   },
   "source": [
    "## Model #3: Add direction of motion to the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mraaBCIkdEwB"
   },
   "source": [
    "TODO: To provide a covariate for direction, create an indicator column,\n",
    "`Direction`, in which each bin contains a 1 if the rat is moving in the\n",
    "positive direction, and 0 otherwise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VL40NTkUCRjO"
   },
   "outputs": [],
   "source": [
    "data['Direction'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4soZTCedghZ"
   },
   "source": [
    "TODO: Add this to the model as another covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAX0HO_GCply"
   },
   "outputs": [],
   "source": [
    "model = sm.GLM(...)\n",
    "\n",
    "results3 = model.fit()\n",
    "print(results3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyRHpfYsds40"
   },
   "source": [
    "**QUESTION (Q9)**: Is the directional coefficient statistically significant?\n",
    "Check the p-values for each predictor. What is the relevant p-value for `rat_direction`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxMRzhDMdyiv"
   },
   "outputs": [],
   "source": [
    "data['Model 3 Cum. Residuals'] = np.cumsum(results3.resid_response)\n",
    "\n",
    "# Superimpose cumulative residuals on rat position over time\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 5))\n",
    "\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]', color = '#D95319', lw = .5, ax = ax)\n",
    "ax.yaxis.label.set_color('#D95319')\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Model 2 Cum. Residuals', color = '#0072BD', lw = .5, ax = ax2)\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Model 3 Cum. Residuals', color = 'k', lw = .5, ax = ax2)\n",
    "\n",
    "# Add a line at 0 to help evaluate our residuals\n",
    "ax2.plot([0, data['Time [s]'].iloc[-1]], [0, 0], 'g', lw = .5)\n",
    "\n",
    "ax2.set_ylabel('Cum. Residuals')\n",
    "ax2.yaxis.label.set_color('#0072BD')\n",
    "\n",
    "fig.legend(['Position', 'Model 2', 'Model 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcxCYnu5fsm5"
   },
   "source": [
    "## Occupancy normalized histogram for each direction of motion separately\n",
    "\n",
    "THOUGHT QUESTION (No LC component)\n",
    "Why might it be useful to fit separate models for each direction of\n",
    "motion. Think about other predictors that we don't have access to. Are\n",
    "there any more predictors we could obtain from the data in our workspace?\n",
    "See if you can obtain any differential statistics of the animal's\n",
    "behavior in the forward and reverse directions. Examine how we can split\n",
    "model fit in forward and reverse directions below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HclC5J8oFs1E"
   },
   "outputs": [],
   "source": [
    "spike_count_up, _ = np.histogram(data['Position [cm]'].loc[data['Spikes'] & (data['Direction'] == 1)], position_bin_edges)\n",
    "position_binned_data['Up Occupancy rate [sp/s]'] = spike_count_up / (position_binned_data['Time / bin [s]'] / 2)\n",
    "\n",
    "spike_count_down, _ = np.histogram(data['Position [cm]'].loc[data['Spikes'] & (data['Direction'] == 0)], position_bin_edges)\n",
    "position_binned_data['Down Occupancy rate [sp/s]'] = spike_count_down / (position_binned_data['Time / bin [s]'] / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEdc3ovDGx5A"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.bar(position_binned_data['Position bins [cm]'], position_binned_data['Up Occupancy rate [sp/s]'], color = 'b', width = 3, label = 'Up')\n",
    "ax.bar(position_binned_data['Position bins [cm]']+3, position_binned_data['Down Occupancy rate [sp/s]'], color = 'r', width = 3, label = 'Down')\n",
    "\n",
    "ax.set(title = 'Occ. nl. h-grams for each direction', \n",
    "       xlabel = 'Position [cm]',\n",
    "       ylabel = 'Occ. nl. rate [sp / s]',\n",
    "       xticks = [0, 50, 100], xticklabels = [0, 50, 100]);\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-a0M_UHKL8G"
   },
   "source": [
    "## Visualize the fit for each direction using glmval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xi134nPIG200"
   },
   "outputs": [],
   "source": [
    "position_binned_data['Up direction'] = np.ones((11,)).astype('float64')\n",
    "position_binned_data['Down direction'] = np.zeros((11,)).astype('float64')\n",
    "\n",
    "up_preds = results3.get_prediction(sm.add_constant(pd.concat((position_binned_data['Position bins [cm]'], \n",
    "                                                                                        position_binned_data['Position bins [cm]']**2,\n",
    "                                                                                        position_binned_data['Up direction']), axis = 1), has_constant = 'add'))\n",
    "\n",
    "position_binned_data['Model 3 predictions: up'] = up_preds.predicted_mean * 1000\n",
    "position_binned_data['Model 3 predictions: up, lower CI'] = up_preds.conf_int()[:, 0] * 1000\n",
    "position_binned_data['Model 3 predictions: up, upper CI'] = up_preds.conf_int()[:, 1] * 1000\n",
    "\n",
    "down_preds = results3.get_prediction(sm.add_constant(pd.concat((position_binned_data['Position bins [cm]'], \n",
    "                                                                                        position_binned_data['Position bins [cm]']**2,\n",
    "                                                                                        position_binned_data['Down direction']), axis = 1), has_constant = 'add'))\n",
    "\n",
    "position_binned_data['Model 3 predictions: down'] = down_preds.predicted_mean * 1000\n",
    "position_binned_data['Model 3 predictions: down, lower CI'] = down_preds.conf_int()[:,0] * 1000\n",
    "position_binned_data['Model 3 predictions: down, upper CI'] = down_preds.conf_int()[:,1] * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uF0x03LBNNWF"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.bar(position_binned_data['Position bins [cm]'], position_binned_data['Up Occupancy rate [sp/s]'], color = 'b', width = 3, label = 'Up')\n",
    "ax.bar(position_binned_data['Position bins [cm]']+3, position_binned_data['Down Occupancy rate [sp/s]'], color = 'r', width = 3, label = 'Down')\n",
    "\n",
    "lower_bound = (position_binned_data['Model 3 predictions: up'] - position_binned_data['Model 3 predictions: up, lower CI']).values\n",
    "upper_bound = (position_binned_data['Model 3 predictions: up, upper CI'] - position_binned_data['Model 3 predictions: up'] ).values\n",
    "ax.errorbar(position_binned_data['Position bins [cm]'], \n",
    "            position_binned_data['Model 3 predictions: up'], \n",
    "            yerr = np.concatenate((lower_bound[:, None], upper_bound[:, None]), axis= 1).T,\n",
    "            color = 'k')\n",
    "\n",
    "lower_bound = (position_binned_data['Model 3 predictions: down'] - position_binned_data['Model 3 predictions: down, lower CI']).values\n",
    "upper_bound = (position_binned_data['Model 3 predictions: down, upper CI'] - position_binned_data['Model 3 predictions: down'] ).values\n",
    "ax.errorbar(position_binned_data['Position bins [cm]'], \n",
    "            position_binned_data['Model 3 predictions: down'], \n",
    "            yerr = np.concatenate((lower_bound[:, None], upper_bound[:, None]), axis= 1).T,\n",
    "            color = 'k')\n",
    "\n",
    "ax.set(title = 'Occ. nl. h-grams for each direction', \n",
    "       xlabel = 'Position [cm]',\n",
    "       ylabel = 'Occ. nl. rate [sp / s]',\n",
    "       xticks = [0, 50, 100], xticklabels = [0, 50, 100]);\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIEAIDa_KW6m"
   },
   "source": [
    "Let's put together everything we've looked at into one figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dLoaXxR2KZ5O"
   },
   "outputs": [],
   "source": [
    "# @markdown Execute to visualize figure\n",
    "fig = plt.figure(constrained_layout=True, figsize = (10, 10))\n",
    "gs = fig.add_gridspec(4, 3)\n",
    "\n",
    "# Position vs time\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]', ax = ax1)\n",
    "sns.scatterplot(data = data.loc[data['Spikes']], x = 'Time [s]', y = 'Position [cm]',\n",
    "                color = 'r', ax = ax1)\n",
    "\n",
    "ax1.set(title = 'Fig. 1: Rat position vs. time');\n",
    "\n",
    "# Spike histogram\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Spike count / bin', ax = ax2)\n",
    "ax2.set(title = 'Spike histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "# Position histogram\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Time / bin [s]', ax = ax3)\n",
    "ax3.set(title = 'Position histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "# Occupancy normalized histogram\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]', ax = ax4)\n",
    "ax4.set(title = 'Occupancy normalized histogram', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "# Model 1\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]', ax = ax5)\n",
    "ax5.plot(position_binned_data['Position bins [cm]']/10, position_binned_data['Model 1 predictions'], 'r')\n",
    "ax5.set(title = 'Model 1: Position only covariate', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "# Model 2\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "sns.barplot(data = position_binned_data, x = 'Position bins [cm]', y = 'Occupancy rate [sp/s]', ax = ax6)\n",
    "ax6.plot(position_binned_data['Position bins [cm]']/10, position_binned_data['Model 2 predictions'], 'r')\n",
    "ax6.set(title = 'Model 2: Add Position^2 as covariate', xticks = [0, 5, 10], xticklabels = [0, 50, 100]);\n",
    "\n",
    "# Direction separated\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.bar(position_binned_data['Position bins [cm]'], position_binned_data['Up Occupancy rate [sp/s]'], color = 'b', width = 3, label = 'Up')\n",
    "ax7.bar(position_binned_data['Position bins [cm]']+3, position_binned_data['Down Occupancy rate [sp/s]'], color = 'r', width = 3, label = 'Down')\n",
    "\n",
    "lower_bound = (position_binned_data['Model 3 predictions: up'] - position_binned_data['Model 3 predictions: up, lower CI']).values\n",
    "upper_bound = (position_binned_data['Model 3 predictions: up, upper CI'] - position_binned_data['Model 3 predictions: up'] ).values\n",
    "ax7.errorbar(position_binned_data['Position bins [cm]'], \n",
    "            position_binned_data['Model 3 predictions: up'], \n",
    "            yerr = np.concatenate((lower_bound[:, None], upper_bound[:, None]), axis= 1).T,\n",
    "            color = 'k')\n",
    "\n",
    "lower_bound = (position_binned_data['Model 3 predictions: down'] - position_binned_data['Model 3 predictions: down, lower CI']).values\n",
    "upper_bound = (position_binned_data['Model 3 predictions: down, upper CI'] - position_binned_data['Model 3 predictions: down'] ).values\n",
    "ax7.errorbar(position_binned_data['Position bins [cm]'], \n",
    "            position_binned_data['Model 3 predictions: down'], \n",
    "            yerr = np.concatenate((lower_bound[:, None], upper_bound[:, None]), axis= 1).T,\n",
    "            color = 'k')\n",
    "\n",
    "ax7.set(title = 'Occ. nl. h-grams for each direction', \n",
    "       xlabel = 'Position [cm]',\n",
    "       ylabel = 'Occ. nl. rate [sp / s]',\n",
    "       xticks = [0, 50, 100], xticklabels = [0, 50, 100]);\n",
    "\n",
    "ax7.legend();\n",
    "\n",
    "# Residuals\n",
    "ax8 = fig.add_subplot(gs[3, :])\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Position [cm]', color = '#D95319', lw = .5, ax = ax8)\n",
    "ax8.yaxis.label.set_color('#D95319')\n",
    "\n",
    "ax9 = plt.twinx()\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Model 2 Cum. Residuals', color = '#0072BD', lw = .5, ax = ax9)\n",
    "sns.lineplot(data = data, x = 'Time [s]', y = 'Model 3 Cum. Residuals', color = 'k', lw = .5, ax = ax9)\n",
    "\n",
    "# Add a line at 0 to help evaluate our residuals\n",
    "ax9.plot([0, data['Time [s]'].iloc[-1]], [0, 0], 'g', lw = .5)\n",
    "\n",
    "ax9.set_ylabel('Cum. Residuals')\n",
    "ax9.yaxis.label.set_color('#0072BD')\n",
    "\n",
    "ax9.legend(['Position', 'Model 2', 'Model 3'])\n",
    "\n",
    "fig.savefig(\"Figure1_ratpos_vs_time.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JdvY8CsKRAV"
   },
   "source": [
    "**QUESTION (Q10)**: Save this figure (fig. #1) as a jpeg, download it from google colab (using next cell), and upload it to Learning Catalytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhqPFX0oqwN7"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"Figure1_ratpos_vs_time.jpg\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaG1uaNjVFxv"
   },
   "source": [
    "# Measures of goodness of fit\n",
    "\n",
    "\"There is not a single procedure for measuring goodness-of-fit; instead\n",
    "there are many tools that, taken together, can provide a broad\n",
    "perspective on the strengths and weaknesses of a set of models.\" \n",
    "- Kramer & Eden 2016, p. 280\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1KLD1itXEYB"
   },
   "source": [
    "## Method 1: Comparing Akaike's Information Criterion (AIC) values\n",
    "\n",
    "AIC is a form of \"penalized likelihood\" measure: we first compute\n",
    "-2*log(likelihood) of the data given the model (will be smaller for\n",
    "better models) and then add a penalty term \"2*p,\" where p is the number\n",
    "of parameters in the model.\n",
    "\n",
    "**QUESTION (Q11)**: Why do we include a penalty for the number of parameters\n",
    "in the model?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dcah1cwSYBi_"
   },
   "source": [
    "\n",
    "Recall that what we are actually predicting is the Poisson rate\n",
    "parameter, lambda. For model #1 (only position as covariate) this is the\n",
    "prediction of the Poisson rate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3KZMiSRXgJd"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Get prediction of Poisson rate parameter\n",
    "lambda1 = np.exp(results1.params[0] + results1.params[1]*data['Position [cm]'])\n",
    "\n",
    "# Could also have done:\n",
    "# lambda11 = results1.fittedvalues\n",
    "\n",
    "# Use 'scipy.stats.poisson.pmf()' to calculate the likelihood of our spiketrain given the\n",
    "# model, then take the log and add up the probabilities:\n",
    "log_likelihood1 = np.sum(np.log(scipy.stats.poisson.pmf(data['Spikes'], lambda1)))\n",
    "\n",
    "# Could have also used:\n",
    "# log_likelihood1 = results1.llf\n",
    "\n",
    "# Calculate AIC for Model 1 (2 parameters)\n",
    "AIC1 = -2 * log_likelihood1 + (2*2)\n",
    "\n",
    "print(AIC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yWvUO6haZL6"
   },
   "source": [
    "Now try it out yourself for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89zsWXzZZTal"
   },
   "outputs": [],
   "source": [
    "# TODO: calculate AIC for Model 2 (3 parameters)\n",
    "... # your code here\n",
    "AIC2 = ...\n",
    "\n",
    "print(AIC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dyh9DPJebSQK"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate the difference in AIC values for Models 1 and 2\n",
    "dAIC = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJ7jZ113bmT2"
   },
   "source": [
    "**QUESTION (Q12)**: What is the difference in AIC values between Model 1 and\n",
    "Model 2?\n",
    "\n",
    "**QUESTION (Q13)**: What does this difference in AIC values mean? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DFKAV7FcArw"
   },
   "source": [
    "NOTE: We actually get AIC for free with `statsmodels1` (but we wanted you to get more familiar with the computation):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KWnR7HncIVb"
   },
   "outputs": [],
   "source": [
    "results1.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoJgNTSXbyfO"
   },
   "source": [
    "NOTE: We can also more easily calculate AIC from the deviance \n",
    "(The deviance is a generalization of the residual sum of squares for all \n",
    "exponential family distributions. Sum of squares is only appropriate for \n",
    "Gaussian distributions.)\n",
    "\n",
    "Compare with dAIC above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBDupUQobjjo"
   },
   "outputs": [],
   "source": [
    "alt_dAIC = (results1.deviance + 2*2) - (results2.deviance + 2*3)    \n",
    "\n",
    "print(alt_dAIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bv8-rtk8cYHO"
   },
   "source": [
    "## Method 2: Confidence intervals on model parameters\n",
    "\n",
    "Recall that the linear regression framework gives us standard errors on\n",
    "our model parameters \"for free.\" We can use these to compute confidence\n",
    "intervals, because maximum likelihood estimators are approximately normal\n",
    "for sufficiently large n.\n",
    "\n",
    "**QUESTION (Q14)**: What is the standard error for beta0 (i.e. the\n",
    "y-intercept) in Model #1? (Hint: the standard error of parameters in statsmodel can be accessed with `results.bse`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDJci2XhcQWi"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate the 95% CI for the parameters of Model 1 using normal\n",
    "# assumptions. HINT: Recall that the linear model is a prediction of\n",
    "# log(lambda). What do we do to interpret our beta's in terms of spike\n",
    "# rate? Note that results1 gives you the confidence intervals but we want you to \n",
    "# actually compute them\n",
    "CI1_low = ...\n",
    "CI1_high = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmvKNzp2djmR"
   },
   "source": [
    "**QUESTION (Q15)**: For Model #1, what is the 95% confidence interval for the\n",
    "neuron's spiking rate at position x=0, in spikes per second?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxQ06mBjeBbc"
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the 95% CI for the parameters of Model 2.\n",
    "CI2_low = ...\n",
    "CI2_high = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlZQJC1kein8"
   },
   "source": [
    "**QUESTION (Q16)**: Based on your 95% CI, can we say that the\n",
    "position-squared term significantly (at alpha < 0.05) improves the model\n",
    "fit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QHtSDS1e74w"
   },
   "source": [
    "We can also perform a direct signifcance test based on each parameter's\n",
    "maximum likelihood estimate and its standard error. This is called the\n",
    "'Wald test', and we also get it for free with statsmodel (results.pvalues)\n",
    "\n",
    "**QUESTION (Q17)**: What is the p-value for the position-squared term in\n",
    "model #2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLJldfzEenOE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xYsFoZofYN2"
   },
   "source": [
    "## Comparing model #3 (with direction term) vs. model #2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2PdkhsafGDU"
   },
   "outputs": [],
   "source": [
    "# Compute different in AIC values between Model 2 and Model 3\n",
    "dAIC = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNvejm_IfakT"
   },
   "source": [
    "**QUESTION (Q18)**: What is the difference in AIC values between Model 2 and Model 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZvDKe-AfotH"
   },
   "outputs": [],
   "source": [
    "# TODO: For model 3, compute 95% CI for last parameter and find the\n",
    "# significance level\n",
    "CI3_low = ...\n",
    "CI3_high = ...\n",
    "p_beta3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZU1-5P16f2aM"
   },
   "source": [
    "**QUESTION (Q19)**: What do these results tell us about our three models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USIiJ1AHgnDL"
   },
   "source": [
    "## Bonus: Kolmogorov-Smirnov plots to evalute models\n",
    "\n",
    "NOTE: There is nothing you have to code in this section, but you should\n",
    "read through it and take a look at the final plots. There is a question\n",
    "for you to answer at the end.\n",
    "\n",
    "Our previous approaches are good for model selection (i.e. adjudicating\n",
    "between two models) or for telling us whether a particular covariate\n",
    "belongs in the model. However, we would also like to know how well our\n",
    "model captures the overall structure of the data. To do this, we will use\n",
    "a method related to the Q-Q plot that we explored in an earlier exercise.\n",
    "You'll recall that this compares the percentiles of our empirical\n",
    "distribution (i.e. the data) to those of some known theoretical\n",
    "distribution (e.g. the standard normal). The K-S plot is a similar idea,\n",
    "but using the cumulative density functions (CDF) as the basis of\n",
    "comparison.\n",
    "\n",
    "However, before we do this, we need to transform our spike data so that\n",
    "they are identically distributed. That is, we need to somehow take into\n",
    "account the fact that lambda is changing over time and as a function of\n",
    "our covariates. To do this, we make use of the \"time-rescaling theorem,\"\n",
    "which essentially sums up all of the lambda_t's between each pair of\n",
    "spikes to produce rescaled waiting times for each of the spikes. If our\n",
    "data are well described by our model, then the re-scaled variables (i.e.\n",
    "the Z's in the code below) will be independent, identically distributed\n",
    "random variables from the exponential distribution with parameter 1. For\n",
    "more details on the math and its application to spiking data see:\n",
    "\n",
    "Brown EN, Barbieri R, Ventura V, Kass RE, Frank LM. The time-rescaling\n",
    "theorem and its application to neural spike train data analysis. Neural\n",
    "Comput. 2002 Feb;14(2):325-46. PubMed PMID: 11802915.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoUpryJGgf33"
   },
   "outputs": [],
   "source": [
    "# Count the total # of spikes\n",
    "n_spikes = data['Spikes'].sum()\n",
    "\n",
    "# Get where spikes occured \n",
    "spike_index = np.where(data['Spikes'].values)[0]\n",
    "\n",
    "# Evaluate Model 2. This is our model's prediction for the value of lambda\n",
    "# (our spike rate parameter) in each time bin.\n",
    "lambda2 = results2.fittedvalues.values\n",
    "\n",
    "# Now we rescale waiting times by summing up the lambdas between each pair\n",
    "# of spikes:\n",
    "Z = np.zeros((n_spikes,))\n",
    "\n",
    "# 1st rescaled waiting time.\n",
    "Z[0] = np.sum(lambda2[0:(spike_index[0]+1)])\n",
    "\n",
    "# and the rest\n",
    "for i in range(1, n_spikes):                      \n",
    "  Z[i] = np.sum(lambda2[spike_index[i-1]:(spike_index[i]+1)])\n",
    "\n",
    "# Next we use 'statsmodels.distributions.empirical_distribution.ecdf' to compute the empirical CDF from rescaled waiting times.\n",
    "ecdf_results = statsmodels.distributions.empirical_distribution.ECDF(Z)\n",
    "ecdf = ecdf_results.y\n",
    "z_vals = ecdf_results.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVchuztD4DfW"
   },
   "source": [
    "Then we compute the theoretical CDF at the same z-values. Recall that the\n",
    "theoretical prediction for the rescaled waiting times is an exponential\n",
    "distribution with parameter 1. (If you're not sure what this last phrase\n",
    "means, go to: https://en.wikipedia.org/wiki/Exponential_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c09rB_Qs39fo"
   },
   "outputs": [],
   "source": [
    "# Model CDF at z values.\n",
    "mcdf = 1 - np.exp(-z_vals)                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOAoXPog4MtQ"
   },
   "source": [
    "Now plot the results. Just as for our Q-Q Plots, if the data area well\n",
    "described by the theoretical distribution, our plot should fall along the\n",
    "y = x line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsewZHyU0uKk"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "\n",
    "ax.plot(mcdf, ecdf, label = 'KS Plot')\n",
    "\n",
    "# Use the normal approximation to calculate CIs (good for n > 20)\n",
    "\n",
    "# Upper confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])+1.96/np.sqrt(n_spikes),'k--', label = '95% CI')\n",
    "\n",
    "# Lower confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])-1.96/np.sqrt(n_spikes),'k--')\n",
    "\n",
    "ax.set(xlabel = 'Model CDF', \n",
    "       ylabel = 'Empirical CDF',\n",
    "       xlim = [0, 1],\n",
    "       ylim = [0, 1],\n",
    "       title = 'KS plot of rescaled data for model #2');\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiPGf93g41PN"
   },
   "source": [
    "Now, let's evaluate model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPoDsG7D0u3h"
   },
   "outputs": [],
   "source": [
    "# Evaluate Model 2. This is our model's prediction for the value of lambda\n",
    "# (our spike rate parameter) in each time bin.\n",
    "lambda3 = results3.fittedvalues.values\n",
    "\n",
    "# Now we rescale waiting times by summing up the lambdas between each pair\n",
    "# of spikes:\n",
    "Z = np.zeros((n_spikes,))\n",
    "\n",
    "# 1st rescaled waiting time.\n",
    "Z[0] = np.sum(lambda3[0:(spike_index[0]+1)])\n",
    "\n",
    "# and the rest\n",
    "for i in range(1, n_spikes):                      \n",
    "  Z[i] = np.sum(lambda3[spike_index[i-1]:(spike_index[i]+1)])\n",
    "\n",
    "# Next we use 'statsmodels.distributions.empirical_distribution.ecdf' to compute the empirical CDF from rescaled waiting times.\n",
    "ecdf_results = statsmodels.distributions.empirical_distribution.ECDF(Z)\n",
    "ecdf3 = ecdf_results.y\n",
    "z_vals3 = ecdf_results.x\n",
    "mcdf3 = 1 - np.exp(-z_vals3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwnnrDPd1Wcm"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize = (10, 10))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(mcdf, ecdf, label = 'KS Plot')\n",
    "\n",
    "# Use the normal approximation to calculate CIs (good for n > 20)\n",
    "\n",
    "# Upper confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])+1.96/np.sqrt(n_spikes),'k--')\n",
    "\n",
    "# Lower confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])-1.96/np.sqrt(n_spikes),'k--', label = '95% CI')\n",
    "\n",
    "ax.set(xlabel = 'Model CDF', \n",
    "       ylabel = 'Empirical CDF',\n",
    "       xlim = [0, 1],\n",
    "       ylim = [0, 1],\n",
    "       title = 'KS plot of rescaled data for model #2');\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mcdf3, ecdf3, label = 'KS Plot')\n",
    "\n",
    "# Use the normal approximation to calculate CIs (good for n > 20)\n",
    "\n",
    "# Upper confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])+1.96/np.sqrt(n_spikes),'k--')\n",
    "\n",
    "# Lower confidence bound.\n",
    "ax.plot([0, 1], np.array([0, 1])-1.96/np.sqrt(n_spikes),'k--')\n",
    "\n",
    "ax.set(xlabel = 'Model CDF', \n",
    "       ylabel = 'Empirical CDF',\n",
    "       xlim = [0, 1],\n",
    "       ylim = [0, 1],\n",
    "       title = 'KS plot of rescaled data for model #3');\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFnsI-by6y-8"
   },
   "source": [
    "**QUESTION (Q20)**: What do you conclude from a comparison of the two Kolmogorov-Smirnov plots?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "placeCellFitEx_ng_student.ipynb",
   "provenance": [
    {
     "file_id": "1xrijqB1mEZuHXpz-0kcskJi8Dk0CMtQw",
     "timestamp": 1632942314338
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
