{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"etDrugRegression_ng_student.ipynb","provenance":[{"file_id":"16TO1KpW0fVl2Ta_TFrw0xyRnCI4rKMRO","timestamp":1632289948962}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yTVGwgA1bMao"},"source":["# TAD Week 4: Drug Regression\n","\n","The goal of this exercise is to introduce you to fitting regression\n","models, to regression diagnostics and to estimating prediction\n","error using cross-validation. In addition, we will re-inforce\n","bootstrapping methods and show how versatile this approach is for\n","calculating standard errors and confidence intervals.\n","\n","The scenario: You are a researcher developing new drugs to treat\n","Amyotrophic Lateral Sclerosis (ALS, a.k.a. \"Lou Gehrig's Disease\"). Prior\n","to testing a new ALS drug in SOD1 mice, you need to develop and test an\n","implantable device to allow chronic delivery of the drug over several\n","days. You contract with a company to produce custom osmotic mini-pumps\n","loaded with drug, and they send you samples of devices from three\n","different manufacturing lots (labeled 'A','B' and 'C'). You implant them\n","in mice for different lengths of time and then remove them and measure\n","the amount of drug remaining in the device. You would like to answer the\n","following questions:\n","\n","1. How is the drug released over time?\n","2. How well might our model predict future data?\n","\n","Other issues to address:\n","Different lots are likely to differ in uninteresting ways, such as the\n","initial amount of drug loaded. How can we prevent this from contaminating\n","our analysis?\n","\n","The data:\n","Each row is data from one animal (n = 27)\n","Column 1 is the manufacturing lot: A, B or C\n","Column 2 is the length of time the device was implanted, in hours\n","Column 3 is the amount of drug *remaining* in the device, in mg.\n","\n","Original source of exercise: Efron, B. & Tibshirani Robert, J. (1993) \n","An introduction to the bootstrap. Chapman & Hall, London u.a. \n","Ex. 9.3 from E & T, Chapter 9, pp. 107 - 112\n","Also now includes cross-validation example from Ch. 17\n","\n","Adapted by RTB, home with the Bubbaloo, 21 Dec. 2016\n","Developed for homework by RAS and RTB, August 2017. Adapted to Python by EB September 2021.\n","\n","What to do: Login to learning catalytics and join the session for the\n","module entitled \"Drug Regression\". You will answer a series of\n","questions based on the guided programming below.  Read through and follow the instructions provided.\n","In some cases you will be asked to answer a question, clearly indicated\n","by 'QUESTION'. In other cases, you be asked to supply missing code,\n","indicated by 'TODO'.\n","\n","**Concepts covered:**\n","1. plotting grouped data \n","2. simple linear regression using sklearn \n","3. simple linear regression using statsmodel\n","4. linear mixed effects models using statsmodel\n","5. two methods for bootstrapping SE estimates: residuals vs. pairs\n","6. regression diagnostics: residuals vs. fitted; q-q plot\n","7. estimating prediction error using cross-validation\n","\n"]},{"cell_type":"code","metadata":{"id":"-NAo1xYgcRw5"},"source":["import numpy as np\n","import scipy.stats\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","matplotlib.rcParams.update({'font.size': 16})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ghXsH79bzoU"},"source":["# Load and plot data"]},{"cell_type":"code","metadata":{"id":"kQKDPgwJa-nl"},"source":["# Load data\n","!gdown --id 1YfCMa4oWfXTWWRQ3C43I-Xbf-zqY_VAT\n","\n","ds = pd.read_excel('/content/DrugData.xlsx')\n","\n","ds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ousXdIHZcefE"},"source":["# Plot data with different symbols for the different lots\n","\n","fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n","sns.scatterplot(data = ds, x = 'hrs', y = 'amount', hue = 'lot', style = 'lot', ax = ax)\n","\n","ax.set(xlabel = 'Time implanted (hrs)', \n","       ylabel = 'Drug remaining (mg)',\n","       title = 'Tests of osmotic mini-pump for drug delivery');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eWA74V4Db6vi"},"source":["**QUESTION (Q1)**: By eye, does the relationship between time implanted and drug remaining look to be linear?\n"]},{"cell_type":"markdown","metadata":{"id":"1VL8ui1zdvuy"},"source":["# Simple linear regression\n","\n","Is there a relationship between the amount of time the drug delivery\n","device was implanted and the amount of drug remaining in the device? (p.\n","108). We will perform a simple linear regression using `sklearn`\n"]},{"cell_type":"code","metadata":{"id":"uw16wuIwbMv0"},"source":["from sklearn import linear_model\n","\n","# Create model\n","reg_model = linear_model.LinearRegression()\n","\n","# Fit model\n","reg_model.fit(X = ds.hrs.values[:, None], y = ds.amount.values)\n","\n","print(reg_model.coef_)\n","print(reg_model.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZTAc1lTv89Sq"},"source":["# Simple linear regression using statsmodel\n","\n","`sklearn` is a very popular package for machine learning algorithms, including regression. It's a little light on statistics though, it doesn't compute confidence intervals, t-statistics, standard errors, etc. We can use a library called `statsmodel` instead, which is what we'll use for the rest of this notebook. This has more useful statistics tools related to regression.\n","\n","We will use `OLS` which stands for ordinary least squares. See the documentation [here](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html)"]},{"cell_type":"code","metadata":{"id":"pzc_3ndxstIr"},"source":["from statsmodels.regression.linear_model import OLS\n","from statsmodels.tools import add_constant\n","\n","# TODO: Use the function 'OLS' to perform the same regression. Hints: add_constant \n","# will add a constant to the input data. \n","lin_reg = OLS(...).fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgD1uygRs_Es"},"source":["You can see a bunch of information about the fit by calling `summary`."]},{"cell_type":"code","metadata":{"id":"TIyxmKjosqoa"},"source":["lin_reg.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTDeeoOys2fP"},"source":["\n","**QUESTION (Q2)**: Do the confidence intervals for our beta coefficients indicate a significant linear relationship between amount of time implanted and the amount of drug remaining in the device? (Hint: this info is contained in the summary above but you can also look at `lin_reg.conf_int()`)\n"]},{"cell_type":"code","metadata":{"id":"XKILa5C0uf82"},"source":["# TODO: extract the value of the y-intercept to a variable called 'b0' and\n","# the slope to 'b1':\n","b0 = ...\n","b1 = ...\n","\n","# Visualize\n","fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n","sns.scatterplot(data = ds, x = 'hrs', y = 'amount', hue = 'lot', style = 'lot', ax = ax)\n","\n","# Plot the regression line\n","xlim = ax.get_xlim()\n","xvals = np.arange(xlim[0], xlim[1], .001)\n","ax.plot(xvals, lin_reg.predict(add_constant(xvals)), 'k')\n","\n","ax.set(xlabel = 'Time implanted (hrs)', \n","       ylabel = 'Drug remaining (mg)',\n","       title = 'Tests of osmotic mini-pump for drug delivery');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtnIXwgTuIKz"},"source":["**QUESTION (Q3)**: What is the regression model estimate of the y-intercept? \n","(Write down the model!!!)\n","\n","**QUESTION (Q4)**: What is its corresponding standard error (can get with `lin_reg.bse`)?\n","\n","**QUESTION (Q5)**: What is H0?\n","\n","**QUESTION (Q6)**: Is our y-intercept value statistically significant at\n","alpha = 0.05?\n","\n","**QUESTION (Q7)**: Do we care about the value of the y-intercept?\n"]},{"cell_type":"markdown","metadata":{"id":"9HftrSwl9e-s"},"source":["# Linear mixed effects: allowing for different y-intercepts\n","\n","NOTE: This section is a freebie: you don't actually have to do anything except run the code. The idea is for you to see how it's done but not to spend too much time figuring out the arcana. But there is a question at the end of the section!\n","\n","\n","Look closely at the first figure in this notebook. Most of the points for Lot C are above the\n","line; most for A and B are below. This suggests that different lots start\n","out with slightly different amounts of drug. We don't really care about\n","this lot-to-lot variation, but it could affect our ability to get a good\n","estimate of the thing we do care about, which is the slope. To a\n","statistician, variables that vary \"just because\" and whose individual\n","labels (e.g. 'Lot A', 'Lot B', 'Lot C') don't have experimental\n","significance are referred to as \"random effects,\" whereas variables\n","that \"matter\" (experimentally speaking) are referred to as \"fixed\n","effects.\" Think of it this way, if the labels of the different lots got\n","switched, it wouldn't much matter to us, but if the 'labels' for the\n","different time points got switched, it would be a disaster. A linear\n","model that allows for both 'fixed' and 'random' effects is called a\n","\"linear mixed effects model.\" We have a fixed effect for the slope (i.e.\n","'hrs') and the intercept, but we also allow a random addition to the\n","intercept for each lot.\n"]},{"cell_type":"code","metadata":{"id":"aGyxyB-Xy8Xj"},"source":["from statsmodels.regression.mixed_linear_model import MixedLM\n","\n","# Use MixedLM to make a linear mixed effects model of the amount of\n","# drug remaining (dependent variable) with a fixed effect of hours\n","# remaining and random effect of lot \n","lme = MixedLM(ds.amount, add_constant(ds.hrs), groups = ds['lot']).fit(reml = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-XlyxpSGm7D"},"source":["Now we need to read out the individual intercepts from the model"]},{"cell_type":"code","metadata":{"id":"jAUxGpOr-Mf4"},"source":["# Give us the fixed effects (slope & intercept)\n","fixed_effects = lme.fe_params\n","\n","# Give us the random effects\n","random_effects = lme.random_effects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8U6xZ639AvDg"},"source":["# Plot the individual regression lines\n","\n","fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n","sns.scatterplot(data = ds, x = 'hrs', y = 'amount', hue = 'lot', style = 'lot', ax = ax)\n","\n","# Plot the regression line\n","xlim = ax.get_xlim()\n","xvals = np.arange(xlim[0], xlim[1], .001)\n","ax.plot(xvals, lin_reg.predict(add_constant(xvals)), 'k', label = 'Lin reg')\n","\n","# Plot specific regression lines\n","y_predictions_A = random_effects['A'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","ax.plot(xvals, y_predictions_A, label = 'A regression')\n","\n","y_predictions_B = random_effects['B'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","ax.plot(xvals, y_predictions_B, label = 'B regression')\n","\n","y_predictions_C = random_effects['C'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","ax.plot(xvals, y_predictions_C, label = 'C regression')\n","\n","ax.legend()\n","ax.set(xlabel = 'Time implanted (hrs)', \n","       ylabel = 'Drug remaining (mg)',\n","       title = 'Tests of osmotic mini-pump for drug delivery');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPPkXmmwJNFV"},"source":["**QUESTION (Q8)**: Only in Matlab.  Do the different lots have different intercepts? To address the issue of lot differences, we can ask whether any of the random effects for the intercept are significantly different from 0. Annoyingly, `statsmodel` doesn't tell us the confidence intervals for the group-based random intercepts so we will skip this question."]},{"cell_type":"markdown","metadata":{"id":"ttSSHxIBKzpH"},"source":["# Regression diagnostics, Part 1\n","\n","What you do AFTER fitting a regression model is every bit as critical as\n","what you do before and during. These diagnostics are important for giving\n","us a visual impression of whether our data meet the assumptions of linear\n","regression:\n","1) independence (each point in scatter plot is independent of others)\n","2) linearity: relationship between x & y is linear\n","3) homoscedasticity of residuals: residuals have the same variance\n","4) normality of the residuals\n","\n","We'll look at two measures:\n","1) Residuals vs. Fitted: The 'fitted' values are the regresion model's\n","prediction of our y's given the actual x's, and the residuals are the\n","differences between our actual y's and our model's predictions.\n","\n","What we want to see: random scatter and no gross departures from linearity and homoscedasticity.\n"]},{"cell_type":"code","metadata":{"id":"ZJsMKbZ7JwGm"},"source":["fig, ax = plt.subplots(1, 1)\n","\n","ax.plot(lin_reg.fittedvalues, lin_reg.resid, 'ko')\n","ax.plot(ax.get_xlim(), [0, 0], 'r')\n","\n","ax.set(xlabel = 'Linear Predictor',\n","       ylabel = 'Residuals', \n","       title = 'Residuals vs Fitted');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwAGrjtpMQaj"},"source":["THOUGHT QUESTIONS: What does the plot of residuals vs. fitted look like? Are our assumptions met?\n","\n","**QUESTION (Q9)**: What do we mean by 'homoscedasticity'?\n"]},{"cell_type":"markdown","metadata":{"id":"IDd44LW-MUo0"},"source":["# Regression diagnostics, Part 2\n","\n","Normal quantile plot (Q-Q Plot) of residuals. Recall that one of the\n","assumptions of regression is that our errors are distributed normally.\n","How can we assess this? While there are a number of statistical tests to\n","assess 'normality' of data, they are all rather weak (that is, they lack\n","statistical power to reject the null when it should be rejected and thus\n","are not very conservative). But a Q-Q plot gives us a good visual.\n","What we want to see: points fall on main diagonal\n"]},{"cell_type":"code","metadata":{"id":"PAgQU-kXNJKL"},"source":["import statsmodels.api as sm\n","fig = sm.qqplot(lin_reg.resid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAlXW54kN5Fa"},"source":["\n","THOUGHT QUESTIONS: What does the Q-Q Plot look like? Are our assumptions\n","met? See the next section for a better intuition on what Q-Q plots look\n","like with matching vs. non-matching distributions.\n","\n","Statistical tests for normality:\n","Lilliefors Test: lillietest\n","Jarque-Bera test: jbtest\n","One-sample Kolmogorov-Smirnov test: kstest\n","Anderson-Darling test: adtest\n","Shapiro-Wilk goodness-of-fit test for normality: download from MathWorks\n","\n","As stated above, the rap against most of these tests is that they are not\n","very powerful, which, in this case, means that they are not very\n","conservative as deciders of normality. How would you get a sense of this\n","by simulation?\n"]},{"cell_type":"markdown","metadata":{"id":"C4-yFtE5OqS_"},"source":["# Bonus on intuition for Q-Q Plot\n","\n","A quanitle-quantile plot (also called a q-q plot) visually assesses\n","whether sample data comes from a specified distribution. Alternatively, a\n","q-q plot assesses whether two sets of sample data come from the same\n","distribution.\n","\n","A q-q plot orders the sample data values from smallest to largest, then\n","plots these values against the expected value for the specified\n","distribution at each quantile in the sample data. The quantile values of\n","the input sample appear along the y-axis, and the theoretical values of\n","the specified distribution at the same quantiles appear along the x-axis.\n","If the resulting plot is linear, then the sample data likely comes from\n","the specified distribution.\n","\n","The q-q plot selects quantiles based on the number of values in the\n","sample data. If the sample data contains n values, then the plot uses n +\n","1 quantiles. Plot the ith ordered value (also called the ith order\n","statistic) against the i (n+1) th quantile of the specified distribution.\n","\n","A q-q plot can also assesses whether two sets of sample data have the\n","same distribution, even if you do not know the underlying distribution.\n","The quantile values for the first data set appear on the x-axis and the\n","corresponding quantile values for the second data set appear on the\n","y-axis. Since q-q plots rely on quantiles, the number of data points in\n","the two samples does not need to be equal. If the sample sizes are\n","unequal, the q-q plot chooses the quantiles based on the smaller data\n","set. If the resulting plot is linear, then the two sets of sample data\n","likely come from the same distribution.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"9FairREM7YNH"},"source":["# Start with a non-normal distribution we have good intuition about:\n","\n","# TODO: Take 10,000 draws from a uniform discrete random distribution with\n","# a maximum of 10 and store it in a variable called 'A'\n","A = ...\n","\n","fig, axes = plt.subplots(3,1, figsize = (10, 10))\n","axes[0].hist(A);\n","\n","# TODO: Make a q-q plot vs. the percentiles in a normal distribution\n","...\n","\n","# TODO: Now use qqplot to compare it to the 'right' distribution (uniform)\n","...\n","\n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4A-HcYLO2QZ"},"source":["# Cross-validation to measure prediction error\n","\n","Calculate the mean residual squared error of our original, simple model (i.e. the one without independent y-intercepts for the different lots).\n","This is just the summed squared error divided by the number of data points.\n"]},{"cell_type":"code","metadata":{"id":"e3P8d50YO3gm"},"source":["mean_RSE = sum(lin_reg.resid**2) / ds.shape[0]\n","print(mean_RSE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TpihcqVPUZz"},"source":["The `mean_RSE` is a measure of how well our model describes the data. But it\n","is overly optimistic, because it is measuring performance using the SAME\n","data that were used to fit the model. That is, the model is optimized to\n","fit precisely this data. But how well would it do on another data set?\n","Well, we could either repeat the experiment, and see how well the model\n","fit to the first data set predicted the new experimental values.\n","Alternatively, we can use cross-validation, in which we divide up the\n","data into K equal-sized parts, fit the model to the other K-1 parts, then\n","measure the error in predicting the Kth part. This is called K-fold\n","cross-validation. A common method is K=n, referred to as \"leave-one-out\"\n","cross-validation.\n","\n","Note that we can no longer use `lin_reg.fittedvalues` to get predicted values, since we want to assess residuals on new data. You can use `lin_reg.predict` to get predictions to inputted data.\n"]},{"cell_type":"code","metadata":{"id":"CTVrahFh8-bV"},"source":["# TODO: Perform a leave-one-out cross-validation in order to calculate \n","#  our CV_residuals\n","\n","n_pts = ds.shape[0] # number of data points\n","CV_residuals = np.zeros((n_pts,))\n","\n","for k in range(n_pts):\n","    ... # your code here!\n","    \n","    # Compute RSE\n","    CV_residuals[k] = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJNaBIoj9Dzl"},"source":["# TODO: Calculate the mean squared error of our cross-validated residuals.\n","CV_mse = ...\n","print(CV_mse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrQzw7kyTNB3"},"source":["**QUESTION (Q10)**: What is the cross-validated mean squared error?\n","\n","**QUESTION (Q11)**: By how many percent does `mean_RSE` underestimate the prediction error as computed by cross-validation? Use `CV_mse` as the gold standard."]},{"cell_type":"code","metadata":{"id":"GI8GRDSIS-HF"},"source":["underest_percent = ...\n","print(underest_percent)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajYPb2lQTfFc"},"source":["# Compare actual residuals with the residuals obtained by cross-validation\n"]},{"cell_type":"code","metadata":{"id":"Zb5cmIZWTa8k"},"source":["fig, ax = plt.subplots(1, 1)\n","\n","ax.plot(ds.hrs, lin_reg.resid, 'ko', label = 'Full-model residuals')\n","ax.plot(ds.hrs, CV_residuals, 'r*', label = 'Cross-validated residuals')\n","ax.set(xlabel = 'Time implanted (hrs)', \n","       ylabel = 'residual',\n","       title = 'Full-model vs. CV Residuals');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8vZQqa1UEMU"},"source":["**THOUGHT QUESTION**: How similar are the actual and cross-validated\n","residuals? In the plot, each asterisk has a corresponding circle. Try to\n","put into words what the difference is between the asterisk and its\n","corresponding circle."]},{"cell_type":"markdown","metadata":{"id":"ee1DKk5-URdm"},"source":["# Other estimates of prediction error\n","\n","As covered on  pp. 242-243 of E&T, other measures of prediction error\n","include adjusted RSE, Akaike Information Criterion (AIC) and Bayesian\n","Information Criterion (BIC). \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"DGFfM48kTvSb"},"source":["# TODO: Read up on 'Akaike Information Criterion', then figure out how to \n","# derive these values for our two models. HINT: We get these for free from\n","# both 'OLS' and `MixedLM`  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KO_IEwgfUma8"},"source":["**QUESTION (Q12)**: Based on the AIC values, which is the \"better\" model?\n"]},{"cell_type":"markdown","metadata":{"id":"KHS9R_nfU5nH"},"source":["# Application of the bootstrap to regression models\n","\n","You might be asking, why do we need to bootstrap standard errors for\n","regression models when we get these (and more) for free from the GLM. The\n","answer is that we don't. BUT, there are plenty of other situations in\n","which we might need to leave the safe, cozy domain of the GLM to\n","situations in which our regression function is non-linear in the\n","parameters (the beta's) or when we want to use a fitting method other\n","than least squares. There is a good example of this in the extra exercise\n","'etCellSurvivalReg.m' where we use 'least median squares' for a more\n","robust fit in the presence of fishy data (i.e. outliers).\n","\n","Classic quote from E&T: \"Thus reassured that the bootstrap is giving\n","reasonable answers in a case we can analyze mathematically, we can go on\n","to apply the bootstrap to more general regression models that have no\n","mathematical solution: where the regression function is non-linear in the\n","parameters beta, and where we use fitting methods other than\n","least-squares.\"\n","\n","So, in the next two sections, we'll use two different bootstrapping\n","approaches for regression models where we can compare our bootstrap\n","estimates of standard errors to those we get from the GLM.\n"]},{"cell_type":"markdown","metadata":{"id":"DmtW4TsZVAQK"},"source":["## Method 1: Bootstrapping residuals\n","\n","The basic idea is that we need an estimate of both the regression\n","coefficients (beta's) and the probability density function (PDF) of the\n","error terms (F). So we use our estimate of beta to calculate the\n","approximate errors, e_i = y_i - BX. These are just the residuals. In\n","other words, our estimate of F is the empirical distribution of the\n","residuals!\n","\n","With OLS in statsmodel, we get estimates of the standard error for each of our coefficients.\n"]},{"cell_type":"code","metadata":{"id":"Jn-IM3rMCixb"},"source":["lin_reg.bse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zcRT4yHYniHb"},"source":["We also get everything we need for the bootstrap:\n","The fitted values (i.e. the predicted values for our actual x-values) (plotted below) and the residuals in `lin_reg.resid`"]},{"cell_type":"code","metadata":{"id":"8aeNsrQ3U6ek"},"source":["# Plot the individual regression lines\n","\n","fig, ax = plt.subplots(1, 1, figsize = (7, 7))\n","sns.scatterplot(data = ds, x = 'hrs', y = 'amount', hue = 'lot', style = 'lot', ax = ax)\n","\n","# Plot the regression line\n","xlim = ax.get_xlim()\n","xvals = np.arange(xlim[0], xlim[1], .001)\n","ax.plot(xvals, lin_reg.predict(add_constant(xvals)), 'k')\n","\n","# Plot specific regression lines\n","y_predictions_A = random_effects['A'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","plt.plot(xvals, y_predictions_A)\n","\n","y_predictions_B = random_effects['B'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","plt.plot(xvals, y_predictions_B)\n","\n","y_predictions_C = random_effects['C'][0] + fixed_effects['const'] + fixed_effects['hrs']*xvals\n","plt.plot(xvals, y_predictions_C)\n","\n","# Plot the predicted values for x-values\n","plt.plot(ds.hrs, lin_reg.fittedvalues, 'ko')\n","\n","ax.set(xlabel = 'Time implanted (hrs)', \n","       ylabel = 'Drug remaining (mg)',\n","       title = 'Tests of osmotic mini-pump for drug delivery');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9GffaC3znlwn"},"source":["TO DO: Write the 'for' loop that will bootstrap the residuals to allow us\n","to estimate our regression coefficients (betas). We've already run the\n","regression on our data above, giving us a model predictions (fitted data)\n","and residuals. In each iteration of the loop, we will randomly resample\n","from the total pool of possible residuals (use `lin_reg.resid`),\n","giving us errorStar (see below). For each of the `n_boot` iterations, we\n","will draw the `n_pts` residuals, which is the number of data points in our\n","original dataset, with replacement. We will add these residuals to the\n","fitted linear predictors of the model to get `y_star`, which we can think of\n","like a 'recomputation' of our data. Another way to think about this is\n","`y_star[i] = beta_hat_0x[i] + error_star[i]` Then, recompute the regression,\n","only with `y_star` instead of the original Y. Store the regression\n","coefficients from each run in a row of `all_beta` (`all_beta` should be a\n","n_boot-x-2 matrix if completed correctly). \n"]},{"cell_type":"code","metadata":{"id":"IHK6zx6mDmfx"},"source":["n_boot = 1000;\n","all_beta = np.zeros((n_boot, 2))\n","\n","np.random.seed(123)\n","for k in range(n_boot):\n","    y_star = ...\n","    \n","    model_fit = OLS(y_star, add_constant(ds.hrs.values)).fit()\n","    all_beta[k, :] = model_fit.params"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJAH5s3npqM-"},"source":["Then we can take the standard deviation of our \"new\" coefficients to estimate standard error."]},{"cell_type":"code","metadata":{"id":"fePF3_uEDw5m"},"source":["bs_SE_resid = ...\n","\n","print(bs_SE_resid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iheoACFGp1hY"},"source":["**THOUGHT QUESTION** (no LC component): Compare `bs_SE_resid` with\n","`lin_reg.bse`. How similar are they?\n","\n","**QUESTION (Q13)**: What is your estimate of the standard error of the slope\n","coefficient based on bootstrapping residuals?\n"]},{"cell_type":"markdown","metadata":{"id":"-nq1WjotcDvg"},"source":["## Method 2: Bootstrapping pairs\n","\n","Instead of resampling residuals and applying them to fitted data, we can\n","select random pairs (or cases) of the data, keeping the x's and y's\n","matched. That is, if we picture the row (observations) by columns\n","(variables) structure of our data, we are randomly sampling rows.\n","We then perform regression on these bootstrapped pairs. "]},{"cell_type":"code","metadata":{"id":"OBKydYz8D3bv"},"source":["np.random.seed(123)\n","for k in range(n_boot):\n","    ... # your code here!\n","    all_beta[k, :] = ...\n","\n","# TODO: Calculate the standard errors for this method:\n","bs_SE_pairs = ...\n","print(bs_SE_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jSS72-E8q3q_"},"source":["**QUESTION (Q14)**: What is your estimate of the standard error of the slope coefficient based on bootstrapping pairs?\n"]},{"cell_type":"markdown","metadata":{"id":"fVHN3FNub-08"},"source":["## Which method is better?\n","\n","E & T state that \"Bootstrapping pairs is less sensitive to assumptions\n","than bootstrapping residuals.\" BS of residuals assumes that the error\n","distribution (i.e. residuals) does not depend on x_i, and this is not\n","always the case. It depends a lot on how good our assumption of linearity\n","is and on the homoscedasticity of the data. See fig. 9.2 on p. 114 of E&T"]},{"cell_type":"code","metadata":{"id":"kLGXYEJMqyPX"},"source":[""],"execution_count":null,"outputs":[]}]}